{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "sys.path.insert(0, os.path.join('..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from radial.batchflow import Dataset, FilesIndex, Pipeline, R, B, V, C\n",
    "from radial.batchflow.models.tf import ResNet18, ResNet34\n",
    "from radial.batchflow.research import Research, Option\n",
    "from radial.core import RadialBatch\n",
    "from radial.core.pipelines import create_preprocess_pipeline, create_train_pipeline\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(path, batch, cross_val=None):\n",
    "    \"\"\"Generated dataset with following conditions: if `cross_val` is None or equal to 1\n",
    "    the one dataset will be created. Elsewhere you will receive list of train and test datasets.\n",
    "    And the lenght of this list will be equal to `cross_val`.\"\"\"\n",
    "    ix = FilesIndex(path=path)\n",
    "    if cross_val is None or cross_val == 1:\n",
    "        dset = Dataset(ix, RadialBatch)\n",
    "        dset.split()\n",
    "        return dset\n",
    "\n",
    "    split_ix = np.array_split(ix.indices, cross_val)\n",
    "    iterations = zip(combinations(split_ix, cross_val-1), list(combinations(split_ix, 1))[::-1])\n",
    "    dsets = []\n",
    "    for train, test in iterations:\n",
    "        dset_train = Dataset(index=ix.create_subset(np.concatenate(train)), batch_class=RadialBatch)\n",
    "        dset_test = Dataset(index=ix.create_subset(np.concatenate(test)), batch_class=RadialBatch)\n",
    "        dsets.append([dset_train, dset_test])\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "если хотите обучать пайплайны по кросс валидации, напишите количество бинов в `cross_val` иначе будет происходить стандартное разделение на train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = 2\n",
    "path = './data/*'\n",
    "\n",
    "dataset = create_datasets(path, RadialBatch, cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание пайплайнов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 50\n",
    "\n",
    "model_config = {\n",
    "    'inputs': dict(points=dict(shape=(2, N_SAMPLES)),\n",
    "                   targets=dict(name='target', shape=1)),\n",
    "    'initial_block/inputs': 'points',\n",
    "    'head': dict(layout='f',\n",
    "                 units=1),\n",
    "    'body/num_blocks': [1, 1, 1],\n",
    "    'body/filters': [8, 16, 32],\n",
    "    'initial_block/filters': C('filters'),\n",
    "    'loss': 'mse',\n",
    "    'optimizer': 'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_SIZE = 50\n",
    "# тут создаем пайплайны для трейна и теста ресерча, без загрузки данных\n",
    "prep_pipeline = create_preprocess_pipeline(N_SAMPLES, np.random.random)\n",
    "train_pipeline = create_train_pipeline(C('model'), model_config, prep_pipeline).run(B_SIZE, n_epochs=None, drop_last=True, lazy=True)\n",
    "\n",
    "test_pipeline = prep_pipeline + (Pipeline()\n",
    "                        .init_variable('predictions', init_on_each_run=list)\n",
    "                        .init_variable('targets', init_on_each_run=list)\n",
    "                        .import_model('model', C('import_from'))\n",
    "                        .update_variable('targets', B('target'), mode='e')\n",
    "                        .predict_model('model', fetches='predictions',\n",
    "                                                feed_dict={'points': B('points'),\n",
    "                                                           'targets': B('target')},\n",
    "                                        save_to=V('predictions'), mode='e')\n",
    "                        .run(B_SIZE, n_epochs=1, drop_last=True, lazy=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики\n",
    "По дефолту будем считать метрики mape и катежеков-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(iteration, experiment, pipeline):\n",
    "    \"\"\" Calculate mean absolute percentage error.\"\"\"\n",
    "    _ = iteration\n",
    "    pipeline = experiment[pipeline].pipeline\n",
    "    y_pred = np.array(pipeline.get_variable('predictions')).reshape(-1)\n",
    "    y_true = np.array(pipeline.get_variable('targets'))\n",
    "    return np.abs(y_true-y_pred)/y_true\n",
    "\n",
    "def get_mape30(iteration, experiment, pipeline):\n",
    "    \"\"\" Calculate percentage of mean absolute percentage error which less than 30%.\"\"\"\n",
    "    mape = mape(iteration, experiment, pipeline)\n",
    "    return np.mean(mape < 0.3)*100\n",
    "\n",
    "# еще сохраним обученные модели на всякий случай\n",
    "def save_model(iteration, experiment, pipeline, model_name, path='./'):\n",
    "    \"\"\" Save model to a path.\"\"\"\n",
    "    path = os.path.join(path, experiment[pipeline].config.alias(as_string=True) + '_' + str(iteration))\n",
    "    pipeline = experiment[pipeline].pipeline\n",
    "    pipeline.save_model(model_name, path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# измени на свои параметры\n",
    "opts = Option('model', [ResNet18, ResNet34]) \\\n",
    "      * Option('filters', [4, 8, 16])\n",
    "\n",
    "research = (Research()\n",
    "            .pipeline(train_pipeline, variables='loss', name='train')\n",
    "            .pipeline(test_pipeline, name='test', execute='%5',\n",
    "                      run=True, import_from='train')\n",
    "            .grid(opts)\n",
    "            .function(get_mape, returns='loss', name='test_mape',\n",
    "                      execute='%5', pipeline='test')\n",
    "            .function(get_mape30, returns='loss', name='test_mape30',\n",
    "                      execute='%5', pipeline='test')\n",
    "            .function(save_model, execute=-1, pipeline='train',\n",
    "                  model_name='model', path='saved_models/')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_research_with_cv(cross_val, res, dataset, n_reps, n_iters, research_name='research'):\n",
    "    \"\"\"execute research with given parameters\"\"\"\n",
    "    research = deepcopy(res)\n",
    "    if not isinstance(dataset, list):\n",
    "        train = Research().pipeline(train_pipeline<<dataset.train, variables='loss', name='train')\n",
    "        test = Research().pipeline(train_pipeline<<dataset.test, name='test', execute='%5', run=True, import_from='train')\n",
    "        research.executables['train'] = train.executables['train']\n",
    "        research.executables['test'] = test.executables['test']\n",
    "        research.run(n_reps=n_reps, n_iters=n_iters, name=research_name, progress_bar=True)\n",
    "        return research\n",
    "\n",
    "    research_list = []\n",
    "    print('number of bins: ', cross_val)\n",
    "    for i in range(cross_val):\n",
    "        research = deepcopy(res)\n",
    "        train = Research().pipeline(train_pipeline<<dataset[i][0], variables='loss', name='train')\n",
    "        test = Research().pipeline(train_pipeline<<dataset[i][1], name='test', execute='%5', run=True, import_from='train')\n",
    "        research.executables['train'] = train.executables['train']\n",
    "        research.executables['test'] = test.executables['test']\n",
    "        research_name_cv = research_name + '_cv_%d' % i\n",
    "        research.run(n_reps=n_reps, n_iters=n_iters, name=research_name_cv, progress_bar=True)\n",
    "        research_list.append(research)\n",
    "    return research_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bins:  2\n",
      "Research research_cv_0 is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor has 6 jobs with 1 iterations. Totally: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:33<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research research_cv_1 is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor has 6 jobs with 1 iterations. Totally: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:32<00:00,  5.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<radial.batchflow.research.research.Research at 0x10e2b3160>,\n",
       " <radial.batchflow.research.research.Research at 0x1c3fb7d400>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_research_with_cv(cross_val, research, dataset, 1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
